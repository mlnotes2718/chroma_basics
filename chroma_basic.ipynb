{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cccbbfa1",
   "metadata": {},
   "source": [
    "# What is ChromaDB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210c4459",
   "metadata": {},
   "source": [
    "Chroma is indeed one of the easiest vector DBs to get started with, but the most widely deployed solutions today tend to be Pinecone and Milvus, with Weaviate and pgvector also very common in production. [dataaspirant](https://dataaspirant.com/popular-vector-databases/)\n",
    "\n",
    "## What “most deployed” usually means\n",
    "\n",
    "When people talk about the “most deployed” vector database, they usually look at a mix of:\n",
    "- Managed SaaS adoption in enterprises (Pinecone). [celerdata](https://celerdata.com/glossary/best-vector-databases)\n",
    "- Open‑source adoption signals like GitHub stars and Docker pulls (Milvus, Qdrant, Weaviate, Chroma, pgvector). [firecrawl](https://www.firecrawl.dev/blog/best-vector-databases-2025)\n",
    "- How often they appear as “default choices” in recent 2025 roundups. [shakudo](https://www.shakudo.io/blog/top-9-vector-databases)\n",
    "\n",
    "By those metrics:\n",
    "- Pinecone is often cited as a leading hosted/vector‑DB service in enterprises. [researchandmarkets](https://www.researchandmarkets.com/reports/6216016/vector-database-global-market-insights)\n",
    "- Milvus is usually the top open‑source option by community size and large‑scale deployments. [dataaspirant](https://dataaspirant.com/popular-vector-databases/)\n",
    "- Weaviate and Qdrant are also heavily used, especially for RAG and multimodal search. [firecrawl](https://www.firecrawl.dev/blog/best-vector-databases-2025)\n",
    "- pgvector is extremely common wherever teams already use Postgres and just add vector search. [dataaspirant](https://dataaspirant.com/popular-vector-databases/)\n",
    "\n",
    "## Where Chroma fits\n",
    "\n",
    "Chroma is frequently recommended when:\n",
    "- You want a simple, embedded or self‑hosted DB for small–medium RAG apps. [liquidmetal](https://liquidmetal.ai/casesAndBlogs/vector-comparison/)\n",
    "- You care more about **ease** and fast iteration than massive multi‑tenant scale. [risingwave](https://risingwave.com/blog/chroma-db-vs-pinecone-vs-faiss-vector-database-showdown/)\n",
    "\n",
    "It’s very popular among individual developers and early‑stage projects, but Pinecone and Milvus are more often named as the most widely deployed in large, production environments. [datainsightsmarket](https://www.datainsightsmarket.com/reports/vector-database-1990919)\n",
    "\n",
    "## Quick guide: choosing in practice\n",
    "\n",
    "- If you want “most common SaaS in production”: Pinecone. [alphamatch](https://www.alphamatch.ai/blog/top-vector-databases-2025)\n",
    "- If you want “most common large‑scale open source”: Milvus (with Qdrant/Weaviate close behind). [shakudo](https://www.shakudo.io/blog/top-9-vector-databases)\n",
    "- If you want “easiest local / small RAG”: Chroma or pgvector. [zilliz](https://zilliz.com/blog/chroma-vs-neo4j-a-comprehensive-vector-database-comparison)\n",
    "\n",
    "- https://cookbook.chromadb.dev/core/api/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76686423",
   "metadata": {},
   "source": [
    "## Best Vector DBs for Production RAG\n",
    "\n",
    "There is no single universal “best” vector DB, but the commonly recommended production options are Milvus, Pinecone, Qdrant, Weaviate, pgvector, and Chroma (mainly for prototyping).\n",
    "\n",
    "### Rule-of-thumb choices\n",
    "\n",
    "- **Pinecone** – Managed SaaS, low-ops, good multi-region, ideal for small–medium scale and teams that want serverless and reliability over cost.\n",
    "- **Milvus** – Strong for 100M–1B+ vectors, self-hosted or Kubernetes, high performance and rich features for large-scale workloads.\n",
    "- **Qdrant** – Cost-effective, Rust-based, good latency and strong metadata filtering for 1M–100M vectors.\n",
    "- **Weaviate** – Good for hybrid search (vector + keyword) and graph-like relations, with a GraphQL API for knowledge-heavy RAG.\n",
    "- **pgvector** – Best when you are already on Postgres and have moderate scale; convenient but not the fastest at huge scale.\n",
    "- **Chroma** – Great DX and simplicity for prototyping and small projects, less common for very large multi-tenant production.\n",
    "\n",
    "### Quick decision table\n",
    "\n",
    "| Situation / requirement                | Recommended DB(s)        |\n",
    "| -------------------------------------- | ------------------------ |\n",
    "| Managed, low DevOps, <100M vectors     | Pinecone                 |\n",
    "| Massive scale (100M–1B+), self-hosted  | Milvus                   |\n",
    "| Tight budget, need strong filters      | Qdrant                   |\n",
    "| Hybrid keyword + vector, relationships | Weaviate                 |\n",
    "| Already on Postgres                    | pgvector                 |\n",
    "| Fast prototyping / PoC                 | Chroma (then migrate)    |\n",
    "\n",
    "### What matters for production RAG\n",
    "\n",
    "Focus on:\n",
    "- Low and stable tail latency (p95).\n",
    "- Predictable scaling behavior.\n",
    "- Strong metadata filtering.\n",
    "- An operational model (SaaS vs self-hosted) your team can realistically maintain.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24e86a4c",
   "metadata": {},
   "source": [
    "# Basics of ChromaDB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e83879f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing results\n",
      "{'ids': [['doc2', 'doc3']], 'embeddings': None, 'documents': [['The dog played in the park', 'Python is a programming language']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[None, None]], 'distances': [[1.3784717321395874, 1.6124422550201416]]}\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# Create a client (this is your database connection)\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Create a collection (think of it like a table)\n",
    "collection = client.create_collection(name=\"my_first_collection\")\n",
    "\n",
    "\n",
    "# Add some documents\n",
    "collection.add(\n",
    "    documents=[\n",
    "        \"The cat sat on the mat\",\n",
    "        \"The dog played in the park\",\n",
    "        \"Python is a programming language\"\n",
    "    ],\n",
    "    ids=[\"doc1\", \"doc2\", \"doc3\"]\n",
    ")\n",
    "\n",
    "# Search for similar documents\n",
    "results = collection.query(\n",
    "    query_texts=[\"Tell me about animals\"],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "print('Showing results')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d34838f1",
   "metadata": {},
   "source": [
    "**The key insight**: ChromaDB automatically converted your text into numbers (called embeddings/vectors). The distance is calculated between these number arrays, not the text itself!\n",
    "\n",
    "**Default settings**:\n",
    "- **Embedding model**: all-MiniLM-L6-v2 (a sentence transformer model)\n",
    "- **Distance metric**: Squared L2 distance (Euclidean distance squared)\n",
    "\n",
    "By default, with `chromadb.Client()`, you're using an in-memory database - it disappears when your program ends!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2abad774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document: ['The cat sat on the mat']\n",
      "\n",
      "Embedding (first 10 numbers): [ 0.13040181 -0.01187006 -0.02811697  0.05123863 -0.05597449  0.03019159\n",
      "  0.03016133  0.02469834 -0.01837056  0.05876682]\n",
      "Embedding length: 384\n"
     ]
    }
   ],
   "source": [
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"test_embeddings\")\n",
    "\n",
    "# Add documents\n",
    "collection.add(\n",
    "    documents=[\"The cat sat on the mat\"],\n",
    "    ids=[\"doc1\"]\n",
    ")\n",
    "\n",
    "# Let's peek at what ChromaDB actually stored\n",
    "results = collection.get(\n",
    "    ids=[\"doc1\"],\n",
    "    include=[\"embeddings\", \"documents\"]\n",
    ")\n",
    "\n",
    "print(\"Document:\", results['documents'])\n",
    "print(\"\\nEmbedding (first 10 numbers):\", results['embeddings'][0][:10])\n",
    "print(\"Embedding length:\", len(results['embeddings'][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb43de17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to /data/my_chroma_db folder\n"
     ]
    }
   ],
   "source": [
    "import chromadb.api\n",
    "\n",
    "chromadb.api.client.SharedSystemClient.clear_system_cache()\n",
    "\n",
    "import chromadb\n",
    "\n",
    "# This creates a local database folder\n",
    "client = chromadb.PersistentClient(path=\"./data/my_chroma_db\")\n",
    "\n",
    "collection = client.get_or_create_collection(name=\"persistent_collection\")\n",
    "\n",
    "collection.add(\n",
    "    documents=[\"This will be saved to disk\"],\n",
    "    ids=[\"doc1\"]\n",
    ")\n",
    "\n",
    "print(\"Data saved to /data/my_chroma_db folder\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f99e2ac8",
   "metadata": {},
   "source": [
    "# ChromaDB File Structure\n",
    "```\n",
    "./my_chroma_db/\n",
    "├── chroma.sqlite3          # Metadata database\n",
    "└── <hash-folder>/          # e.g., 4f2a3b1c-...\n",
    "    ├── data_level0.bin     # Vector data\n",
    "    ├── header.bin          # Index metadata\n",
    "    ├── length.bin          # Document lengths\n",
    "    └── link_lists.bin      # Graph connections (for HNSW)\n",
    "```\n",
    "\n",
    "## 1. `chroma.sqlite3` - The Metadata Store\n",
    "\n",
    "This SQLite database stores:\n",
    "- Collection names and settings\n",
    "- Document IDs\n",
    "- Document text (the actual strings you added)\n",
    "- Metadata (any extra info you attached)\n",
    "- Configuration (which embedding function, distance metric, etc.)\n",
    "\n",
    "**Think of it as:** The \"catalog\" or \"index card system\" that keeps track of what you have\n",
    "\n",
    "## 2. The Hash Folder - The Vector Store\n",
    "\n",
    "Each collection gets a folder (named with a UUID hash). Inside are `.bin` files that store:\n",
    "\n",
    "- **`data_level0.bin`**: Your actual embedding vectors (those arrays of floats)\n",
    "- **`header.bin`**: Metadata about the index structure\n",
    "- **`length.bin`**: Information about vector dimensions\n",
    "- **`link_lists.bin`**: The HNSW (Hierarchical Navigable Small World) graph structure for fast similarity search\n",
    "\n",
    "## Key Concepts\n",
    "\n",
    "### Distance Calculation\n",
    "\n",
    "✅ Distance is calculated **on-the-fly** during search  \n",
    "✅ If a document is never queried, no distance is ever calculated for it  \n",
    "✅ Distance only exists **between two vectors** (query vector ↔ document vector)\n",
    "\n",
    "### Summary Table\n",
    "\n",
    "| What | Where Stored | When Created |\n",
    "|------|--------------|--------------|\n",
    "| **Document text** | `chroma.sqlite3` | When you `.add()` |\n",
    "| **Document vectors** | `data_level0.bin` | When you `.add()` (via embedding function) |\n",
    "| **Query vector** | Nowhere (temporary) | When you `.query()` (on-the-fly) |\n",
    "| **Distance** | Nowhere | When you `.query()` (calculated on-the-fly) |\n",
    "\n",
    "## Performance Optimization\n",
    "\n",
    "The `.bin` files like `link_lists.bin` help speed up distance calculation by using the HNSW (Hierarchical Navigable Small World) algorithm, which allows ChromaDB to search efficiently without calculating distances to all vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3e5e43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Showing results\n",
      "{'ids': [['doc1']], 'embeddings': None, 'documents': [['This will be saved to disk']], 'uris': None, 'included': ['metadatas', 'documents', 'distances'], 'data': None, 'metadatas': [[None]], 'distances': [[1.9052947759628296]]}\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "# This creates a local database folder\n",
    "client = chromadb.PersistentClient(path=\"./data/my_chroma_db\")\n",
    "\n",
    "collection = client.get_or_create_collection(name=\"persistent_collection\")\n",
    "\n",
    "# Search for similar documents\n",
    "results = collection.query(\n",
    "    query_texts=[\"Tell me about animals\"],\n",
    "    n_results=2\n",
    ")\n",
    "\n",
    "print('Showing results')\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3a403efd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documents in collection: ['This will be saved to disk']\n",
      "Number of documents: 1\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./data/my_chroma_db\")\n",
    "collection = client.get_or_create_collection(name=\"persistent_collection\")\n",
    "\n",
    "# Let's see what's actually IN the collection\n",
    "all_docs = collection.get()\n",
    "print(\"Documents in collection:\", all_docs['documents'])\n",
    "print(\"Number of documents:\", len(all_docs['documents']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c6c94e04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Query: 'Tell me about animals' ---\n",
      "1. Distance: 1.3785 - 'The dog played in the park'\n",
      "2. Distance: 1.6124 - 'Python is a programming language'\n",
      "3. Distance: 1.8603 - 'The cat sat on the mat'\n",
      "\n",
      "--- Query: 'cat' ---\n",
      "1. Distance: 0.9752 - 'The cat sat on the mat'\n",
      "2. Distance: 1.5271 - 'Python is a programming language'\n",
      "3. Distance: 1.6214 - 'The dog played in the park'\n",
      "\n",
      "--- Query: 'pets and animals' ---\n",
      "1. Distance: 1.1869 - 'The dog played in the park'\n",
      "2. Distance: 1.5547 - 'The cat sat on the mat'\n",
      "3. Distance: 1.6321 - 'Python is a programming language'\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"animal_test\")\n",
    "\n",
    "collection.add(\n",
    "    documents=[\n",
    "        \"The cat sat on the mat\",\n",
    "        \"The dog played in the park\",\n",
    "        \"Python is a programming language\"\n",
    "    ],\n",
    "    ids=[\"doc1\", \"doc2\", \"doc3\"]\n",
    ")\n",
    "\n",
    "# Let's try different queries\n",
    "queries = [\n",
    "    \"Tell me about animals\",\n",
    "    \"cat\",\n",
    "    \"pets and animals\"\n",
    "]\n",
    "\n",
    "for query in queries:\n",
    "    results = collection.query(query_texts=[query], n_results=3)\n",
    "    print(f\"\\n--- Query: '{query}' ---\")\n",
    "    for i, (doc, distance) in enumerate(zip(results['documents'][0], results['distances'][0])):\n",
    "        print(f\"{i+1}. Distance: {distance:.4f} - '{doc}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cc569e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Query: 'animal' ---\n",
      "  0.0000 - 'animal'\n",
      "  0.3688 - 'dog'\n",
      "  0.6511 - 'cat'\n",
      "  0.7057 - 'Tell me about animals'\n",
      "  1.5166 - 'The cat sat on the mat'\n",
      "\n",
      "--- Query: 'animals' ---\n",
      "  0.3075 - 'animal'\n",
      "  0.4693 - 'Tell me about animals'\n",
      "  0.7699 - 'dog'\n",
      "  0.9449 - 'cat'\n",
      "  1.6997 - 'The cat sat on the mat'\n",
      "\n",
      "--- Query: 'cat' ---\n",
      "  0.0000 - 'cat'\n",
      "  0.6511 - 'animal'\n",
      "  0.6787 - 'dog'\n",
      "  0.9752 - 'The cat sat on the mat'\n",
      "  1.3497 - 'Tell me about animals'\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(name=\"word_test\")\n",
    "\n",
    "# Let's test single words vs. sentences\n",
    "collection.add(\n",
    "    documents=[\n",
    "        \"cat\",\n",
    "        \"dog\", \n",
    "        \"animal\",\n",
    "        \"The cat sat on the mat\",\n",
    "        \"Tell me about animals\"\n",
    "    ],\n",
    "    ids=[\"word_cat\", \"word_dog\", \"word_animal\", \"sentence_cat\", \"question_animals\"]\n",
    ")\n",
    "\n",
    "# Compare these queries\n",
    "test_queries = [\"animal\", \"animals\", \"cat\"]\n",
    "\n",
    "for query in test_queries:\n",
    "    results = collection.query(query_texts=[query], n_results=5)\n",
    "    print(f\"\\n--- Query: '{query}' ---\")\n",
    "    for doc, dist in zip(results['documents'][0], results['distances'][0]):\n",
    "        print(f\"  {dist:.4f} - '{doc}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd807c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from huggingface_hub import login # Or other HF libraries\n",
    "\n",
    "load_dotenv() # Loads variables from .env\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "59f86cbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Now use the token with HF libraries, e.g.:\n",
    "login(token=hf_token)\n",
    "# Or directly in from_pretrained:\n",
    "# model = AutoModel.from_pretrained(\"meta-llama/Llama-2-7b-chat-hf\", token=hf_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "468273cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.2.2\n"
     ]
    }
   ],
   "source": [
    "import sentence_transformers\n",
    "print(sentence_transformers.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31770c42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e3416b648d841609a6c86362fa3a7f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/229 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49a64270c0074514ad288b8dea1f2a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/122 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8807884166de431ca401403d367154af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f3239b8697345f1b7cb13a81e618c91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b19d63b3834f67b1a03a2fa1b0ec61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/629 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b101425944843fea067da7948bee1a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74f692259594481eb9dcbe2b8a2a3a54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/103 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "BertModel LOAD REPORT from: sentence-transformers/paraphrase-MiniLM-L6-v2\n",
      "Key                     | Status     |  | \n",
      "------------------------+------------+--+-\n",
      "embeddings.position_ids | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3091eee2bae54dc784361ee57576e298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b243ab2ff3b34b6aab1f2e9a9abaaade",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e488f0924594abeb4535d8a07b45fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e7e16563a81495c829f11c1f9850dc2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f73ef1eaca11432da02d3093604d770e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "\n",
    "# Example: Using OpenAI embeddings (you'll replace this with your own)\n",
    "# openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "#     api_key=\"your-api-key\",\n",
    "#     model_name=\"text-embedding-ada-002\"\n",
    "# )\n",
    "\n",
    "# Or using SentenceTransformers with a different model\n",
    "sentence_transformer_ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=\"paraphrase-MiniLM-L6-v2\"\n",
    ")\n",
    "\n",
    "client = chromadb.Client()\n",
    "collection = client.create_collection(\n",
    "    name=\"custom_embeddings\",\n",
    "    embedding_function=sentence_transformer_ef\n",
    ")\n",
    "\n",
    "collection.add(\n",
    "    documents=[\"The cat sat on the mat\"],\n",
    "    ids=[\"doc1\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fdb51fd7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba7dad7502d444a8863e9f0c29e2852f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "581b98700a834b778a6b95f12d25271d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a06d13f029df4416addd075255b36354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2a524c9e59d41a7b726f11785552364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "195ee470457d4d9790f388cfce26e2d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "562e8c5f2cfa45918ae4ad5a04619811",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a81562eb35b84c4e92331fb2c81a4d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading weights:   0%|          | 0/148 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel LOAD REPORT from: gpt2\n",
      "Key                  | Status     |  | \n",
      "---------------------+------------+--+-\n",
      "h.{0...11}.attn.bias | UNEXPECTED |  | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "404786534f2b4ab1949f794788d347e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens (IDs): [464, 3797, 3332, 319, 262, 2603]\n",
      "Tokens (text): ['The', ' cat', ' sat', ' on', ' the', ' mat']\n",
      "\n",
      "What's inside the model:\n",
      "- Number of hidden layers: 13\n",
      "- Shape of last hidden state: torch.Size([1, 6, 768])\n",
      "- That means: [batch_size, sequence_length, embedding_dimension]\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "model_name = 'gpt2'\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
    "model = GPT2LMHeadModel.from_pretrained(model_name)\n",
    "\n",
    "text = \"The cat sat on the mat\"\n",
    "\n",
    "# Step 1: Tokenization\n",
    "tokens = tokenizer.encode(text)\n",
    "print(\"Tokens (IDs):\", tokens)\n",
    "print(\"Tokens (text):\", [tokenizer.decode([t]) for t in tokens])\n",
    "\n",
    "# Step 2: What happens inside the model?\n",
    "input_ids = torch.tensor([tokens])\n",
    "with torch.no_grad():\n",
    "    outputs = model(input_ids, output_hidden_states=True)\n",
    "\n",
    "print(\"\\nWhat's inside the model:\")\n",
    "print(\"- Number of hidden layers:\", len(outputs.hidden_states))\n",
    "print(\"- Shape of last hidden state:\", outputs.hidden_states[-1].shape)\n",
    "print(\"- That means: [batch_size, sequence_length, embedding_dimension]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e44e61a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results: [['The dog played in the park', 'The cat sat on the mat', 'Python is a programming language']]\n",
      "Distances: [[0.7549513578414917, 0.77315354347229, 0.864280104637146]]\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "from chromadb.utils import embedding_functions\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv() # Loads variables from .env\n",
    "openai_token = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not openai_token:\n",
    "    raise ValueError(\"OPENAI_API_KEY environment variable is not set\")\n",
    "\n",
    "# Create OpenAI embedding function\n",
    "openai_ef = embedding_functions.OpenAIEmbeddingFunction(\n",
    "    api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model_name=\"text-embedding-3-small\"  # or \"text-embedding-3-large\"\n",
    ")\n",
    "\n",
    "# Create persistent client with OpenAI embeddings\n",
    "client = chromadb.PersistentClient(path=\"./data/openai_chroma_db\")\n",
    "\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"openai_collection\",\n",
    "    embedding_function=openai_ef\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "collection.add(\n",
    "    documents=[\n",
    "        \"The cat sat on the mat\",\n",
    "        \"The dog played in the park\",\n",
    "        \"Python is a programming language\"\n",
    "    ],\n",
    "    ids=[\"doc1\", \"doc2\", \"doc3\"]\n",
    ")\n",
    "\n",
    "# Query\n",
    "results = collection.query(\n",
    "    query_texts=[\"Tell me about animals\"],\n",
    "    #n_results=2\n",
    ")\n",
    "\n",
    "print(\"Results:\", results['documents'])\n",
    "print(\"Distances:\", results['distances'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c462ab5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for doc1 (first 5 numbers): [ 0.13040181 -0.01187006 -0.02811697  0.05123863 -0.05597449]\n",
      "Vector length: 384\n",
      "\n",
      "Distance to doc1: 0.9752493500709534\n",
      "Distance to doc2: 1.3566614389419556\n"
     ]
    }
   ],
   "source": [
    "import chromadb\n",
    "\n",
    "client = chromadb.PersistentClient(path=\"./data/test_db\")\n",
    "collection = client.get_or_create_collection(name=\"test\")\n",
    "\n",
    "collection.add(\n",
    "    documents=[\"The cat sat on the mat\", \"The dog played\"],\n",
    "    ids=[\"doc1\", \"doc2\"]\n",
    ")\n",
    "\n",
    "# Get the raw data\n",
    "result = collection.get(ids=[\"doc1\"], include=[\"embeddings\"])\n",
    "print(\"Vector for doc1 (first 5 numbers):\", result['embeddings'][0][:5])\n",
    "print(\"Vector length:\", len(result['embeddings'][0]))\n",
    "\n",
    "# Now query\n",
    "query_result = collection.query(\n",
    "    query_texts=[\"cat\"],\n",
    "    n_results=2,\n",
    "    include=[\"embeddings\", \"distances\"]\n",
    ")\n",
    "print(\"\\nDistance to doc1:\", query_result['distances'][0][0])\n",
    "print(\"Distance to doc2:\", query_result['distances'][0][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32fbb5e",
   "metadata": {},
   "source": [
    "- https://claude.ai/share/c86070ea-4fa0-4c73-bcf2-f3f635f042f0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2f391a",
   "metadata": {},
   "source": [
    "## End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e45a00",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "chromadb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
