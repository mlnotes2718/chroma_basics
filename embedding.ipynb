{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1960dc91",
   "metadata": {},
   "source": [
    "# What is Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a738a3",
   "metadata": {},
   "source": [
    "## Summary: Embeddings in NLP (Traditional ‚Üí Neural ‚Üí Transformer)\n",
    "\n",
    "### 1. What is an Embedding?\n",
    "An **embedding** is a numerical (dense vector) representation of a discrete object (e.g., a word) that captures useful information for machine learning models.\n",
    "\n",
    "- Purpose: Convert symbols ‚Üí numbers\n",
    "- Meaning is encoded in **geometry (distances & directions)**, not individual values\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What is a Word Embedding?\n",
    "A **word embedding** maps each word to a vector such that:\n",
    "- Semantically similar words are close in vector space\n",
    "- Dissimilar words are far apart\n",
    "\n",
    "Examples:\n",
    "- Word2Vec\n",
    "- GloVe\n",
    "- FastText\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Embedding Dimension: Minimum and Maximum\n",
    "\n",
    "#### Minimum\n",
    "- **Theoretical minimum:** 1 (not useful)\n",
    "- **Practical minimum:** ~10‚Äì50 (toy or simple tasks)\n",
    "\n",
    "#### Maximum\n",
    "- **Theoretical maximum:** Unlimited\n",
    "- **Practical range:**\n",
    "  - Static embeddings: 100‚Äì300\n",
    "  - Transformers: 768‚Äì12,000+\n",
    "\n",
    "Trade-off:\n",
    "- Too small ‚Üí insufficient capacity\n",
    "- Too large ‚Üí overfitting, inefficiency\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Meaning of Each Number in an Embedding\n",
    "\n",
    "Key truth:\n",
    "> **Individual embedding dimensions have no fixed human-interpretable meaning.**\n",
    "\n",
    "- Embeddings are **distributed representations**\n",
    "- Meaning emerges from:\n",
    "  - Distances\n",
    "  - Angles\n",
    "  - Directions between vectors\n",
    "- Dimensions are **rotation-invariant** and arbitrary\n",
    "\n",
    "Meaning lives in **relationships**, not coordinates.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. How Are Embeddings Trained?\n",
    "\n",
    "#### Core principle\n",
    "**Distributional hypothesis**:\n",
    "> Words appearing in similar contexts have similar meanings.\n",
    "\n",
    "#### Neural embeddings (most common)\n",
    "- Trained via **gradient descent**\n",
    "- Objective: Predict context or next word\n",
    "- Examples:\n",
    "  - Word2Vec (Skip-gram, CBOW)\n",
    "  - FastText\n",
    "  - BERT, GPT\n",
    "\n",
    "#### Non-gradient embeddings\n",
    "- LSA (SVD-based)\n",
    "- Spectral embeddings\n",
    "- TF-IDF (not really embeddings)\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Traditional NLP vs Transformer NLP\n",
    "\n",
    "#### Traditional NLP\n",
    "- Representations:\n",
    "  - Bag-of-Words\n",
    "  - TF-IDF\n",
    "  - N-grams\n",
    "- Position:\n",
    "  - N-grams\n",
    "  - Sliding windows\n",
    "- Interaction:\n",
    "  - Hand-crafted features\n",
    "  - RNN/LSTM recurrence\n",
    "- Pipelines were **modular and non end-to-end**\n",
    "\n",
    "#### Transformer NLP\n",
    "- Token embeddings\n",
    "- Positional embeddings\n",
    "- Attention mechanism\n",
    "- End-to-end learned representations\n",
    "- Contextual embeddings\n",
    "\n",
    "**Key difference:**\n",
    "> Traditional NLP separates meaning, position, and interaction; transformers learn them jointly.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Are All Embeddings Trained with Gradient Descent?\n",
    "\n",
    "**No.**\n",
    "\n",
    "| Embedding Type | Gradient Descent |\n",
    "|---|---|\n",
    "| Word2Vec | Yes |\n",
    "| FastText | Yes |\n",
    "| GloVe | Yes |\n",
    "| BERT / GPT | Yes |\n",
    "| LSA | No |\n",
    "| TF-IDF | No |\n",
    "| Spectral embeddings | No |\n",
    "| Random embeddings | No |\n",
    "\n",
    "Modern NLP is dominated by gradient-based methods due to scalability and flexibility.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Embeddings in RNN / LSTM Models\n",
    "\n",
    "Key point:\n",
    "> **Embeddings are crucial in RNN/LSTM-based NLP models.**\n",
    "\n",
    "- Embeddings provide semantic signal\n",
    "- RNN/LSTM:\n",
    "  - Models order\n",
    "  - Aggregates information over time\n",
    "- Cannot compensate for poor embeddings (GIGO principle)\n",
    "\n",
    "Empirical finding:\n",
    "- Pre-trained embeddings often contribute more than the RNN itself\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Pre-trained Embeddings: Frozen vs Fine-Tuned\n",
    "\n",
    "Pre-trained embeddings are **initializations**, not fixed by default.\n",
    "\n",
    "#### Option 1: Frozen\n",
    "- No updates during training\n",
    "- Good for small datasets\n",
    "- Faster, more stable\n",
    "\n",
    "#### Option 2: Fine-tuned\n",
    "- Updated via backpropagation\n",
    "- Adapts to task/domain\n",
    "- Risk of overfitting or forgetting\n",
    "\n",
    "Best practice:\n",
    "- Small data ‚Üí freeze\n",
    "- Larger or domain-specific data ‚Üí fine-tune (often with smaller LR)\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Big Picture Takeaways\n",
    "\n",
    "- Embeddings turn language into geometry\n",
    "- Individual dimensions are meaningless; geometry is everything\n",
    "- Gradient descent dominates modern embedding learning\n",
    "- In RNN/LSTM models, embeddings carry most semantic power\n",
    "- Transformers reduce reliance on static embeddings via attention\n",
    "- Pre-trained embeddings can (and often should) be fine-tuned\n",
    "\n",
    "---\n",
    "\n",
    "### One-Sentence Summary\n",
    "\n",
    "**Embeddings are learned geometric representations of language; how powerful your NLP model is largely depends on how well those vectors encode meaning and context.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4b76e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Skip-gram with a Simple Numerical Example\n",
    "\n",
    "I‚Äôll tell it like it is:\n",
    "\n",
    "Real skip-gram training uses gradient descent and softmax over large vocabularies. Doing full training by hand is ugly.  \n",
    "So we‚Äôll use a **tiny toy corpus**, **2D embeddings**, and show **one concrete skip-gram update intuition with numbers**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1Ô∏è‚É£ Tiny corpus (toy data)\n",
    "\n",
    "**Sentence:**\n",
    "\n",
    "> ‚ÄúI like cats‚Äù\n",
    "\n",
    "**Vocabulary (index ‚Üí word):**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3562e5",
   "metadata": {},
   "source": [
    "\n",
    "**Window size = 1**\n",
    "\n",
    "### Skip-gram pairs (center ‚Üí context)\n",
    "\n",
    "| Center | Context |\n",
    "|------|--------|\n",
    "| like | I |\n",
    "| like | cats |\n",
    "\n",
    "So the model learns:\n",
    "\n",
    "> If the center word is **‚Äúlike‚Äù**, it should predict **‚ÄúI‚Äù** and **‚Äúcats‚Äù**\n",
    "\n",
    "---\n",
    "\n",
    "## 2Ô∏è‚É£ What skip-gram actually learns (no magic)\n",
    "\n",
    "Skip-gram learns **two embeddings per word**:\n",
    "\n",
    "- **Input vector** (center word)\n",
    "- **Output vector** (context word)\n",
    "\n",
    "We‚Äôll use **2D embeddings** to visualize easily.\n",
    "\n",
    "---\n",
    "\n",
    "## 3Ô∏è‚É£ Initialize embeddings (random, small numbers)\n",
    "\n",
    "### Input vectors (V)\n",
    "\n",
    "| Word | Vector |\n",
    "|----|------|\n",
    "| I | `[0.2, 0.1]` |\n",
    "| like | `[0.0, 0.3]` |\n",
    "| cats | `[0.4, 0.2]` |\n",
    "\n",
    "### Output vectors (U)\n",
    "\n",
    "| Word | Vector |\n",
    "|----|------|\n",
    "| I | `[0.1, 0.0]` |\n",
    "| like | `[0.0, 0.2]` |\n",
    "| cats | `[0.3, 0.1]` |\n",
    "\n",
    "---\n",
    "\n",
    "## 4Ô∏è‚É£ Skip-gram prediction (core calculation)\n",
    "\n",
    "We take **center word = \"like\"**\n",
    "\n",
    "### Step 1: Dot product with each context word\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "\\[\n",
    "\\text{score}(w_c, w_o) = \\mathbf{v}_{w_c} \\cdot \\mathbf{u}_{w_o}\n",
    "\\]\n",
    "\n",
    "### Dot products\n",
    "\n",
    "**score(like, I):**\n",
    "\n",
    "\\[\n",
    "[0.0, 0.3] \\cdot [0.1, 0.0] = 0.0\n",
    "\\]\n",
    "\n",
    "**score(like, cats):**\n",
    "\n",
    "\\[\n",
    "[0.0, 0.3] \\cdot [0.3, 0.1] = 0.03\n",
    "\\]\n",
    "\n",
    "**score(like, like):**\n",
    "\n",
    "\\[\n",
    "[0.0, 0.3] \\cdot [0.0, 0.2] = 0.06\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 5Ô∏è‚É£ Convert scores to probabilities (softmax)\n",
    "\n",
    "\\[\n",
    "P(w_o \\mid w_c) =\n",
    "\\frac{e^{\\text{score}}}{\\sum e^{\\text{scores}}}\n",
    "\\]\n",
    "\n",
    "### Exponentials\n",
    "\n",
    "| Word | exp(score) |\n",
    "|----|----|\n",
    "| I | \\(e^0 = 1\\) |\n",
    "| cats | \\(e^{0.03} \\approx 1.03\\) |\n",
    "| like | \\(e^{0.06} \\approx 1.06\\) |\n",
    "\n",
    "**Sum ‚âà 3.09**\n",
    "\n",
    "### Probabilities\n",
    "\n",
    "| Word | Probability |\n",
    "|----|----|\n",
    "| I | \\(1 / 3.09 \\approx 0.32\\) |\n",
    "| cats | \\(1.03 / 3.09 \\approx 0.33\\) |\n",
    "| like | \\(1.06 / 3.09 \\approx 0.35\\) |\n",
    "\n",
    "‚ùå **Problem:**  \n",
    "The model predicts **‚Äúlike‚Äù** as context, which is wrong.\n",
    "\n",
    "---\n",
    "\n",
    "## 6Ô∏è‚É£ What learning does (plain English)\n",
    "\n",
    "Skip-gram will:\n",
    "\n",
    "- Pull **‚Äúlike‚Äù** closer to **‚ÄúI‚Äù** and **‚Äúcats‚Äù**\n",
    "- Push **‚Äúlike‚Äù** away from unrelated words\n",
    "\n",
    "After many updates, embeddings might look like this:\n",
    "\n",
    "---\n",
    "\n",
    "## 7Ô∏è‚É£ Final learned 2D embeddings (intuitive result)\n",
    "\n",
    "### Input embeddings (after training)\n",
    "\n",
    "| Word | Vector |\n",
    "|----|------|\n",
    "| I | `[-0.2, 0.3]` |\n",
    "| like | `[0.0, 0.5]` |\n",
    "| cats | `[0.2, 0.4]` |\n",
    "\n",
    "### Visual intuition (2D space)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89855d35",
   "metadata": {},
   "source": [
    "\n",
    "‚úî Words appearing together are **geometrically close**\n",
    "\n",
    "---\n",
    "\n",
    "## 8Ô∏è‚É£ Simple analogy (remember this)\n",
    "\n",
    "- Skip-gram is like **learning a map of words**\n",
    "- Words that appear together are **neighbors on the map**\n",
    "- Training = repeatedly nudging words **closer or farther apart**\n",
    "\n",
    "No linguistics.  \n",
    "No grammar rules.  \n",
    "Just **geometry + statistics**.\n",
    "\n",
    "---\n",
    "\n",
    "## 9Ô∏è‚É£ Key takeaway (don‚Äôt sugar-coat it)\n",
    "\n",
    "- Word embeddings are **not semantic by design**\n",
    "- Meaning **emerges from co-occurrence**\n",
    "- Skip-gram is just:\n",
    "\n",
    "> ‚ÄúAdjust vectors so dot products predict nearby words‚Äù\n",
    "\n",
    "---\n",
    "\n",
    "## üìö References (foundational, authoritative)\n",
    "\n",
    "- Mikolov et al., *Efficient Estimation of Word Representations in Vector Space*, 2013  \n",
    "- Goldberg & Levy, *word2vec Explained*, 2014  \n",
    "- Jurafsky & Martin, *Speech and Language Processing*, Chapter on Vector Semantics  \n",
    "\n",
    "---\n",
    "\n",
    "If you want next, I can:\n",
    "\n",
    "- Show **negative sampling numerically**\n",
    "- Compare **CBOW vs Skip-gram**\n",
    "- Plot this in **Python**\n",
    "- Explain why embeddings capture **analogies (king ‚àí man + woman)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e40b8",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
