{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4b16714",
   "metadata": {},
   "source": [
    "# Application of VectorDB and Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628fe3dd",
   "metadata": {},
   "source": [
    "# TL;DR: Token Embeddings vs Vector Databases\n",
    "\n",
    "## Key Distinction\n",
    "\n",
    "**Token/Word Embeddings** = Tool used to create vectors  \n",
    "**Vector Database** = Storage for final document vectors\n",
    "\n",
    "---\n",
    "\n",
    "## Where Are Token Embeddings Stored?\n",
    "\n",
    "**Inside the model weight files** (e.g., `pytorch_model.bin`, `model.safetensors`)\n",
    "```python\n",
    "# Token embeddings are model parameters\n",
    "model.embeddings.word_embeddings.weight\n",
    "# Shape: [vocab_size, embedding_dim]\n",
    "# Example: [50,000 tokens Ã— 768 dimensions]\n",
    "```\n",
    "\n",
    "**Physical location:** Model checkpoint files (~440 MB for BERT)\n",
    "\n",
    "---\n",
    "\n",
    "## Vector DB vs Model Training\n",
    "\n",
    "### Training a Transformer (No Vector DB)\n",
    "- Train from scratch on raw text\n",
    "- Embeddings learned and stored in **model weights**\n",
    "- Output: Trained model file\n",
    "- **NO VECTOR DB INVOLVED**\n",
    "\n",
    "### RAG Application (Uses Vector DB)\n",
    "- Use **pre-trained** model\n",
    "- Generate embeddings for documents\n",
    "- Store document vectors in **Vector DB** (Pinecone, Weaviate, Chroma)\n",
    "- Retrieve relevant docs at query time\n",
    "\n",
    "---\n",
    "\n",
    "## What Goes in Vector DB?\n",
    "\n",
    "**NOT stored:** Individual token embeddings  \n",
    "**Stored:** Document/chunk embeddings\n",
    "```\n",
    "Input: \"Machine learning is powerful\"\n",
    "â†“\n",
    "Model processes using token embeddings internally\n",
    "â†“\n",
    "Output: Single vector [0.42, 0.28, 0.35, ...] â† Stored in Vector DB\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Simple Analogy\n",
    "\n",
    "- **Token embeddings** = Knife (tool)\n",
    "- **Document embeddings** = Chopped vegetables (product stored in container)\n",
    "- **Vector DB** = Container/refrigerator (storage system)\n",
    "- **Model training** = Learning to use the knife (no storage needed)\n",
    "- **RAG** = Using the knife to prep food, then storing it (storage needed)\n",
    "\n",
    "---\n",
    "\n",
    "## Bottom Line\n",
    "\n",
    "1. Token embeddings live **inside model files** as learned parameters\n",
    "2. Vector DBs store **document embeddings** created by those models\n",
    "3. Training = no Vector DB; RAG = uses Vector DB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1960dc91",
   "metadata": {},
   "source": [
    "# What is Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32a738a3",
   "metadata": {},
   "source": [
    "## Summary: Embeddings in NLP (Traditional â†’ Neural â†’ Transformer)\n",
    "\n",
    "### 1. What is an Embedding?\n",
    "An **embedding** is a numerical (dense vector) representation of a discrete object (e.g., a word) that captures useful information for machine learning models.\n",
    "\n",
    "- Purpose: Convert symbols â†’ numbers\n",
    "- Meaning is encoded in **geometry (distances & directions)**, not individual values\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What is a Word Embedding?\n",
    "A **word embedding** maps each word to a vector such that:\n",
    "- Semantically similar words are close in vector space\n",
    "- Dissimilar words are far apart\n",
    "\n",
    "Examples:\n",
    "- Word2Vec\n",
    "- GloVe\n",
    "- FastText\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Embedding Dimension: Minimum and Maximum\n",
    "\n",
    "#### Minimum\n",
    "- **Theoretical minimum:** 1 (not useful)\n",
    "- **Practical minimum:** ~10â€“50 (toy or simple tasks)\n",
    "\n",
    "#### Maximum\n",
    "- **Theoretical maximum:** Unlimited\n",
    "- **Practical range:**\n",
    "  - Static embeddings: 100â€“300\n",
    "  - Transformers: 768â€“12,000+\n",
    "\n",
    "Trade-off:\n",
    "- Too small â†’ insufficient capacity\n",
    "- Too large â†’ overfitting, inefficiency\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Meaning of Each Number in an Embedding\n",
    "\n",
    "Key truth:\n",
    "> **Individual embedding dimensions have no fixed human-interpretable meaning.**\n",
    "\n",
    "- Embeddings are **distributed representations**\n",
    "- Meaning emerges from:\n",
    "  - Distances\n",
    "  - Angles\n",
    "  - Directions between vectors\n",
    "- Dimensions are **rotation-invariant** and arbitrary\n",
    "\n",
    "Meaning lives in **relationships**, not coordinates.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. How Are Embeddings Trained?\n",
    "\n",
    "#### Core principle\n",
    "**Distributional hypothesis**:\n",
    "> Words appearing in similar contexts have similar meanings.\n",
    "\n",
    "#### Neural embeddings (most common)\n",
    "- Trained via **gradient descent**\n",
    "- Objective: Predict context or next word\n",
    "- Examples:\n",
    "  - Word2Vec (Skip-gram, CBOW)\n",
    "  - FastText\n",
    "  - BERT, GPT\n",
    "\n",
    "#### Non-gradient embeddings\n",
    "- LSA (SVD-based)\n",
    "- Spectral embeddings\n",
    "- TF-IDF (not really embeddings)\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Traditional NLP vs Transformer NLP\n",
    "\n",
    "#### Traditional NLP\n",
    "- Representations:\n",
    "  - Bag-of-Words\n",
    "  - TF-IDF\n",
    "  - N-grams\n",
    "- Position:\n",
    "  - N-grams\n",
    "  - Sliding windows\n",
    "- Interaction:\n",
    "  - Hand-crafted features\n",
    "  - RNN/LSTM recurrence\n",
    "- Pipelines were **modular and non end-to-end**\n",
    "\n",
    "#### Transformer NLP\n",
    "- Token embeddings\n",
    "- Positional embeddings\n",
    "- Attention mechanism\n",
    "- End-to-end learned representations\n",
    "- Contextual embeddings\n",
    "\n",
    "**Key difference:**\n",
    "> Traditional NLP separates meaning, position, and interaction; transformers learn them jointly.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Are All Embeddings Trained with Gradient Descent?\n",
    "\n",
    "**No.**\n",
    "\n",
    "| Embedding Type | Gradient Descent |\n",
    "|---|---|\n",
    "| Word2Vec | Yes |\n",
    "| FastText | Yes |\n",
    "| GloVe | Yes |\n",
    "| BERT / GPT | Yes |\n",
    "| LSA | No |\n",
    "| TF-IDF | No |\n",
    "| Spectral embeddings | No |\n",
    "| Random embeddings | No |\n",
    "\n",
    "Modern NLP is dominated by gradient-based methods due to scalability and flexibility.\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Embeddings in RNN / LSTM Models\n",
    "\n",
    "Key point:\n",
    "> **Embeddings are crucial in RNN/LSTM-based NLP models.**\n",
    "\n",
    "- Embeddings provide semantic signal\n",
    "- RNN/LSTM:\n",
    "  - Models order\n",
    "  - Aggregates information over time\n",
    "- Cannot compensate for poor embeddings (GIGO principle)\n",
    "\n",
    "Empirical finding:\n",
    "- Pre-trained embeddings often contribute more than the RNN itself\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Pre-trained Embeddings: Frozen vs Fine-Tuned\n",
    "\n",
    "Pre-trained embeddings are **initializations**, not fixed by default.\n",
    "\n",
    "#### Option 1: Frozen\n",
    "- No updates during training\n",
    "- Good for small datasets\n",
    "- Faster, more stable\n",
    "\n",
    "#### Option 2: Fine-tuned\n",
    "- Updated via backpropagation\n",
    "- Adapts to task/domain\n",
    "- Risk of overfitting or forgetting\n",
    "\n",
    "Best practice:\n",
    "- Small data â†’ freeze\n",
    "- Larger or domain-specific data â†’ fine-tune (often with smaller LR)\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Big Picture Takeaways\n",
    "\n",
    "- Embeddings turn language into geometry\n",
    "- Individual dimensions are meaningless; geometry is everything\n",
    "- Gradient descent dominates modern embedding learning\n",
    "- In RNN/LSTM models, embeddings carry most semantic power\n",
    "- Transformers reduce reliance on static embeddings via attention\n",
    "- Pre-trained embeddings can (and often should) be fine-tuned\n",
    "\n",
    "---\n",
    "\n",
    "### One-Sentence Summary\n",
    "\n",
    "**Embeddings are learned geometric representations of language; how powerful your NLP model is largely depends on how well those vectors encode meaning and context.**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66f4b76e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Skip-gram with a Simple Numerical Example\n",
    "\n",
    "Iâ€™ll tell it like it is:\n",
    "\n",
    "Real skip-gram training uses gradient descent and softmax over large vocabularies. Doing full training by hand is ugly.  \n",
    "So weâ€™ll use a **tiny toy corpus**, **2D embeddings**, and show **one concrete skip-gram update intuition with numbers**.\n",
    "\n",
    "---\n",
    "\n",
    "## 1ï¸âƒ£ Tiny corpus (toy data)\n",
    "\n",
    "**Sentence:**\n",
    "\n",
    "> â€œI like catsâ€\n",
    "\n",
    "**Vocabulary (index â†’ word):**\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b3562e5",
   "metadata": {},
   "source": [
    "\n",
    "**Window size = 1**\n",
    "\n",
    "### Skip-gram pairs (center â†’ context)\n",
    "\n",
    "| Center | Context |\n",
    "|------|--------|\n",
    "| like | I |\n",
    "| like | cats |\n",
    "\n",
    "So the model learns:\n",
    "\n",
    "> If the center word is **â€œlikeâ€**, it should predict **â€œIâ€** and **â€œcatsâ€**\n",
    "\n",
    "---\n",
    "\n",
    "## 2ï¸âƒ£ What skip-gram actually learns (no magic)\n",
    "\n",
    "Skip-gram learns **two embeddings per word**:\n",
    "\n",
    "- **Input vector** (center word)\n",
    "- **Output vector** (context word)\n",
    "\n",
    "Weâ€™ll use **2D embeddings** to visualize easily.\n",
    "\n",
    "---\n",
    "\n",
    "## 3ï¸âƒ£ Initialize embeddings (random, small numbers)\n",
    "\n",
    "### Input vectors (V)\n",
    "\n",
    "| Word | Vector |\n",
    "|----|------|\n",
    "| I | `[0.2, 0.1]` |\n",
    "| like | `[0.0, 0.3]` |\n",
    "| cats | `[0.4, 0.2]` |\n",
    "\n",
    "### Output vectors (U)\n",
    "\n",
    "| Word | Vector |\n",
    "|----|------|\n",
    "| I | `[0.1, 0.0]` |\n",
    "| like | `[0.0, 0.2]` |\n",
    "| cats | `[0.3, 0.1]` |\n",
    "\n",
    "---\n",
    "\n",
    "## 4ï¸âƒ£ Skip-gram prediction (core calculation)\n",
    "\n",
    "We take **center word = \"like\"**\n",
    "\n",
    "### Step 1: Dot product with each context word\n",
    "\n",
    "**Formula:**\n",
    "\n",
    "\\[\n",
    "\\text{score}(w_c, w_o) = \\mathbf{v}_{w_c} \\cdot \\mathbf{u}_{w_o}\n",
    "\\]\n",
    "\n",
    "### Dot products\n",
    "\n",
    "**score(like, I):**\n",
    "\n",
    "\\[\n",
    "[0.0, 0.3] \\cdot [0.1, 0.0] = 0.0\n",
    "\\]\n",
    "\n",
    "**score(like, cats):**\n",
    "\n",
    "\\[\n",
    "[0.0, 0.3] \\cdot [0.3, 0.1] = 0.03\n",
    "\\]\n",
    "\n",
    "**score(like, like):**\n",
    "\n",
    "\\[\n",
    "[0.0, 0.3] \\cdot [0.0, 0.2] = 0.06\n",
    "\\]\n",
    "\n",
    "---\n",
    "\n",
    "## 5ï¸âƒ£ Convert scores to probabilities (softmax)\n",
    "\n",
    "\\[\n",
    "P(w_o \\mid w_c) =\n",
    "\\frac{e^{\\text{score}}}{\\sum e^{\\text{scores}}}\n",
    "\\]\n",
    "\n",
    "### Exponentials\n",
    "\n",
    "| Word | exp(score) |\n",
    "|----|----|\n",
    "| I | \\(e^0 = 1\\) |\n",
    "| cats | \\(e^{0.03} \\approx 1.03\\) |\n",
    "| like | \\(e^{0.06} \\approx 1.06\\) |\n",
    "\n",
    "**Sum â‰ˆ 3.09**\n",
    "\n",
    "### Probabilities\n",
    "\n",
    "| Word | Probability |\n",
    "|----|----|\n",
    "| I | \\(1 / 3.09 \\approx 0.32\\) |\n",
    "| cats | \\(1.03 / 3.09 \\approx 0.33\\) |\n",
    "| like | \\(1.06 / 3.09 \\approx 0.35\\) |\n",
    "\n",
    "âŒ **Problem:**  \n",
    "The model predicts **â€œlikeâ€** as context, which is wrong.\n",
    "\n",
    "---\n",
    "\n",
    "## 6ï¸âƒ£ What learning does (plain English)\n",
    "\n",
    "Skip-gram will:\n",
    "\n",
    "- Pull **â€œlikeâ€** closer to **â€œIâ€** and **â€œcatsâ€**\n",
    "- Push **â€œlikeâ€** away from unrelated words\n",
    "\n",
    "After many updates, embeddings might look like this:\n",
    "\n",
    "---\n",
    "\n",
    "## 7ï¸âƒ£ Final learned 2D embeddings (intuitive result)\n",
    "\n",
    "### Input embeddings (after training)\n",
    "\n",
    "| Word | Vector |\n",
    "|----|------|\n",
    "| I | `[-0.2, 0.3]` |\n",
    "| like | `[0.0, 0.5]` |\n",
    "| cats | `[0.2, 0.4]` |\n",
    "\n",
    "### Visual intuition (2D space)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89855d35",
   "metadata": {},
   "source": [
    "\n",
    "âœ” Words appearing together are **geometrically close**\n",
    "\n",
    "---\n",
    "\n",
    "## 8ï¸âƒ£ Simple analogy (remember this)\n",
    "\n",
    "- Skip-gram is like **learning a map of words**\n",
    "- Words that appear together are **neighbors on the map**\n",
    "- Training = repeatedly nudging words **closer or farther apart**\n",
    "\n",
    "No linguistics.  \n",
    "No grammar rules.  \n",
    "Just **geometry + statistics**.\n",
    "\n",
    "---\n",
    "\n",
    "## 9ï¸âƒ£ Key takeaway (donâ€™t sugar-coat it)\n",
    "\n",
    "- Word embeddings are **not semantic by design**\n",
    "- Meaning **emerges from co-occurrence**\n",
    "- Skip-gram is just:\n",
    "\n",
    "> â€œAdjust vectors so dot products predict nearby wordsâ€\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š References (foundational, authoritative)\n",
    "\n",
    "- Mikolov et al., *Efficient Estimation of Word Representations in Vector Space*, 2013  \n",
    "- Goldberg & Levy, *word2vec Explained*, 2014  \n",
    "- Jurafsky & Martin, *Speech and Language Processing*, Chapter on Vector Semantics  \n",
    "\n",
    "---\n",
    "\n",
    "If you want next, I can:\n",
    "\n",
    "- Show **negative sampling numerically**\n",
    "- Compare **CBOW vs Skip-gram**\n",
    "- Plot this in **Python**\n",
    "- Explain why embeddings capture **analogies (king âˆ’ man + woman)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f7e40b8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "29d943b8",
   "metadata": {},
   "source": [
    "# Document Embedding: Complete Guide\n",
    "\n",
    "## Table of Contents\n",
    "1. [What is Document Embedding](#what-is-document-embedding)\n",
    "2. [How to Use Pre-trained Models](#how-to-use-pre-trained-models)\n",
    "3. [Applications Beyond Search](#applications-beyond-search)\n",
    "4. [Vector Database Use Cases](#vector-database-use-cases)\n",
    "5. [Critical Considerations](#critical-considerations)\n",
    "\n",
    "---\n",
    "\n",
    "## What is Document Embedding\n",
    "\n",
    "### Definition\n",
    "Document embedding converts entire documents (articles, emails, paragraphs) into a single vector of numbers that captures its meaning.\n",
    "\n",
    "**Analogy**: Like a fingerprint for text - documents with similar meanings get similar fingerprints.\n",
    "\n",
    "### Basic Process\n",
    "```\n",
    "\"The cat sat on the mat\" â†’ [0.23, -0.45, 0.67, 0.12, ..., -0.31]\n",
    "```\n",
    "\n",
    "Typical dimensions: 300-1536 (or more)\n",
    "\n",
    "### Why It Matters\n",
    "- **Find similar documents** - semantic grouping\n",
    "- **Search by meaning** - not just keywords\n",
    "- **Cluster documents** - automatic categorization\n",
    "- **Feed into ML models** - convert text to numbers\n",
    "\n",
    "### Simple Example\n",
    "```\n",
    "Document 1: \"dogs are pets\" â†’ [0.8, 0.1, 0.2]\n",
    "Document 2: \"cats are pets\" â†’ [0.7, 0.2, 0.3]\n",
    "Document 3: \"cars need fuel\" â†’ [0.1, 0.9, 0.5]\n",
    "\n",
    "similarity(Doc1, Doc2) = 0.97  # very similar\n",
    "similarity(Doc1, Doc3) = 0.52  # less similar\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## How to Use Pre-trained Models\n",
    "\n",
    "### Key Concept\n",
    "Pre-trained models don't need prior embedding information - they create embeddings from raw text.\n",
    "\n",
    "**Analogy**: Like a meat grinder - feed raw text in, get embeddings out. The model was already \"trained\" to process text properly.\n",
    "\n",
    "### Basic Workflow\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# 1. Load pre-trained model\n",
    "model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "# 2. Your raw documents\n",
    "documents = [\n",
    "    \"Machine learning is a subset of AI\",\n",
    "    \"Deep learning uses neural networks\"\n",
    "]\n",
    "\n",
    "# 3. Generate embeddings\n",
    "def get_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", \n",
    "                      padding=True, truncation=True)\n",
    "    outputs = model(**inputs)\n",
    "    embedding = outputs.last_hidden_state.mean(dim=1)\n",
    "    return embedding.squeeze().numpy()\n",
    "\n",
    "embeddings = [get_embedding(doc) for doc in documents]\n",
    "```\n",
    "\n",
    "### Internal Process\n",
    "1. **Tokenization**: Text â†’ Token IDs\n",
    "2. **Model processing**: Token IDs â†’ Contextual representations\n",
    "3. **Pooling**: Multiple token vectors â†’ Single document vector\n",
    "\n",
    "---\n",
    "\n",
    "## Applications Beyond Search\n",
    "\n",
    "### 1. Document Classification\n",
    "Train classifiers on embeddings to categorize documents.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(train_embeddings, labels)\n",
    "prediction = classifier.predict([new_embedding])\n",
    "```\n",
    "\n",
    "**Use cases**: Email filtering, content moderation, support ticket routing\n",
    "\n",
    "### 2. Clustering & Topic Discovery\n",
    "Group similar documents without predefined categories.\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "kmeans = KMeans(n_clusters=10)\n",
    "clusters = kmeans.fit_predict(embeddings)\n",
    "```\n",
    "\n",
    "**Use cases**: Organizing document collections, discovering themes in feedback\n",
    "\n",
    "### 3. Anomaly Detection\n",
    "Find documents that don't fit normal patterns.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "detector = IsolationForest()\n",
    "detector.fit(normal_embeddings)\n",
    "is_anomaly = detector.predict([new_embedding])\n",
    "```\n",
    "\n",
    "**Use cases**: Fraud detection, spotting unusual reports, content moderation\n",
    "\n",
    "### 4. Recommendation Systems\n",
    "Recommend documents similar to user's interests.\n",
    "\n",
    "```python\n",
    "user_profile = np.mean([get_embedding(doc) for doc in user_history], axis=0)\n",
    "similarities = cosine_similarity([user_profile], all_embeddings)\n",
    "top_recommendations = np.argsort(similarities[0])[-5:]\n",
    "```\n",
    "\n",
    "**Use cases**: News recommendations, research paper suggestions\n",
    "\n",
    "### 5. Information Extraction & QA\n",
    "Find which document/section contains the answer.\n",
    "\n",
    "```python\n",
    "q_embedding = get_embedding(question)\n",
    "similarities = cosine_similarity([q_embedding], section_embeddings)\n",
    "best_section = sections[np.argmax(similarities)]\n",
    "```\n",
    "\n",
    "**Use cases**: Customer support bots, legal document analysis\n",
    "\n",
    "### 6. Duplicate Detection\n",
    "Find near-duplicate or plagiarized content.\n",
    "\n",
    "```python\n",
    "similarity = cosine_similarity([doc1_emb], [doc2_emb])[0][0]\n",
    "if similarity > 0.95:\n",
    "    print(\"Likely duplicate\")\n",
    "```\n",
    "\n",
    "**Use cases**: Content moderation, plagiarism checking, deduplication\n",
    "\n",
    "### 7. Transfer Learning\n",
    "Use embeddings as features for other ML tasks.\n",
    "\n",
    "```python\n",
    "model = RandomForestRegressor()\n",
    "model.fit(embeddings, engagement_scores)\n",
    "predicted_engagement = model.predict([new_emb])\n",
    "```\n",
    "\n",
    "**Use cases**: Predicting virality, reading time estimation\n",
    "\n",
    "### 8. Multi-modal Applications\n",
    "Combine with image/audio embeddings.\n",
    "\n",
    "**Use cases**: Image captioning, video search, cross-lingual search\n",
    "\n",
    "### 9. Data Augmentation\n",
    "Generate synthetic examples by interpolating embeddings.\n",
    "\n",
    "```python\n",
    "synthetic_emb = 0.5 * emb1 + 0.5 * emb2\n",
    "```\n",
    "\n",
    "**Use cases**: Expanding training datasets, data balancing\n",
    "\n",
    "---\n",
    "\n",
    "## Vector Database Use Cases\n",
    "\n",
    "### Beyond RAG Applications\n",
    "\n",
    "#### 1. Recommendation Systems\n",
    "Store user and item embeddings for real-time recommendations.\n",
    "\n",
    "```python\n",
    "# Find similar products\n",
    "results = collection.query(\n",
    "    query_embeddings=[user_interest],\n",
    "    n_results=5\n",
    ")\n",
    "```\n",
    "\n",
    "**Real use**: Netflix shows, Spotify songs, Amazon products, LinkedIn jobs\n",
    "\n",
    "**Reference**: Pinterest uses vector search for visual recommendations (Pinterest Engineering Blog, 2019)\n",
    "\n",
    "#### 2. Duplicate/Near-Duplicate Detection\n",
    "Find similar content at scale.\n",
    "\n",
    "```python\n",
    "duplicates = collection.query(\n",
    "    query_embeddings=[new_embedding],\n",
    "    where={\"status\": \"open\"}\n",
    ")\n",
    "```\n",
    "\n",
    "**Real use**: Content moderation, legal document review, code plagiarism\n",
    "\n",
    "**Reference**: Airbnb uses embeddings for duplicate listing detection (KDD 2018)\n",
    "\n",
    "#### 3. Anomaly/Fraud Detection\n",
    "Identify outliers in high-dimensional space.\n",
    "\n",
    "```python\n",
    "avg_distance = np.mean(neighbors[\"distances\"])\n",
    "if avg_distance > threshold:\n",
    "    flag_as_fraud()\n",
    "```\n",
    "\n",
    "**Real use**: Credit card fraud, insurance claims, network intrusion, quality control\n",
    "\n",
    "**Reference**: PayPal uses embeddings for fraud detection (IEEE Symposium, 2016)\n",
    "\n",
    "#### 4. Personalization & User Modeling\n",
    "Store and update user preference vectors over time.\n",
    "\n",
    "```python\n",
    "# Update with exponential moving average\n",
    "new_emb = (1 - weight) * current_emb + weight * action_embedding\n",
    "```\n",
    "\n",
    "**Real use**: News feed personalization, adaptive learning, dating apps\n",
    "\n",
    "#### 5. Image/Multi-modal Search\n",
    "Search images by visual similarity or text description.\n",
    "\n",
    "```python\n",
    "# Search images using text query\n",
    "text_embedding = model.encode_text(\"red sports car\")\n",
    "results = image_collection.query(query_embeddings=[text_embedding])\n",
    "```\n",
    "\n",
    "**Real use**: Google Images, Pinterest visual search, e-commerce product search\n",
    "\n",
    "**Reference**: Google uses embeddings for image search (CVPR 2020)\n",
    "\n",
    "#### 6. Version Control & Change Detection\n",
    "Track how documents/code evolves.\n",
    "\n",
    "```python\n",
    "# Compare consecutive versions\n",
    "if similarity < 0.7:\n",
    "    print(\"Major change detected\")\n",
    "```\n",
    "\n",
    "**Real use**: Wiki changes, code review prioritization, contract tracking\n",
    "\n",
    "#### 7. A/B Testing & Content Optimization\n",
    "Analyze what content performs well.\n",
    "\n",
    "```python\n",
    "# Predict performance based on similar content\n",
    "predicted_performance = np.mean([m[\"open_rate\"] for m in similar])\n",
    "```\n",
    "\n",
    "**Real use**: Email marketing, headline testing, ad copy optimization\n",
    "\n",
    "#### 8. Clustering & Segmentation\n",
    "Dynamically group similar items.\n",
    "\n",
    "```python\n",
    "similar_customers = collection.query(\n",
    "    query_embeddings=[seed_customer.embedding],\n",
    "    where={\"churn_risk\": \"high\"}\n",
    ")\n",
    "```\n",
    "\n",
    "**Real use**: Customer segmentation, market research, user cohorts\n",
    "\n",
    "#### 9. Time-Series Pattern Matching\n",
    "Find similar patterns in sequential data.\n",
    "\n",
    "```python\n",
    "pattern_emb = embed_timeseries(price_pattern)\n",
    "similar_patterns = collection.query(query_embeddings=[pattern_emb])\n",
    "```\n",
    "\n",
    "**Real use**: Trading patterns, predictive maintenance, weather forecasting\n",
    "\n",
    "**Reference**: Uber uses embeddings for time-series forecasting (Uber Engineering Blog, 2020)\n",
    "\n",
    "#### 10. Cold Start Problem Solutions\n",
    "Match new users/items based on attributes.\n",
    "\n",
    "```python\n",
    "new_user_emb = get_embedding(str(user_attributes))\n",
    "similar_users = collection.query(query_embeddings=[new_user_emb])\n",
    "```\n",
    "\n",
    "**Real use**: New user onboarding, new product recommendations\n",
    "\n",
    "### Vector DB Advantages\n",
    "1. **Speed**: Sub-second search on millions/billions of vectors\n",
    "2. **Scale**: Handle datasets too large for memory\n",
    "3. **Updates**: Real-time insertion/deletion\n",
    "4. **Filtering**: Combine similarity with metadata filters\n",
    "5. **Hybrid Search**: Mix keyword and semantic search\n",
    "\n",
    "### Popular Vector Databases\n",
    "- **Pinecone**: Managed, easy to use\n",
    "- **Weaviate**: Open-source, ML-native\n",
    "- **Milvus**: Scalable, open-source\n",
    "- **Qdrant**: Rust-based, fast\n",
    "- **ChromaDB**: Simple, embedded\n",
    "\n",
    "**References**:\n",
    "- Malkov & Yashunin (2018). \"Efficient and robust approximate nearest neighbor search using HNSW\"\n",
    "- Johnson et al. (2019). \"Billion-scale similarity search with GPUs\" - FAISS\n",
    "\n",
    "---\n",
    "\n",
    "## Critical Considerations\n",
    "\n",
    "### 1. Different Embedding Methods & Trade-offs\n",
    "\n",
    "#### a) Averaging Word Embeddings (Old School)\n",
    "```python\n",
    "word_embeddings = [w2v_model[word] for word in doc.split()]\n",
    "doc_embedding = np.mean(word_embeddings, axis=0)\n",
    "```\n",
    "- **Pro**: Fast, lightweight\n",
    "- **Con**: Loses word order\n",
    "- **When**: Simple tasks, limited compute\n",
    "\n",
    "#### b) Transformer-based (Modern Standard)\n",
    "```python\n",
    "doc_embedding = model.encode(doc)\n",
    "```\n",
    "- **Pro**: Captures context, word order, semantics\n",
    "- **Con**: Slower, larger models\n",
    "- **When**: Most production use cases\n",
    "\n",
    "#### c) Specialized Models\n",
    "- **Dense Passage Retrieval (DPR)**: Optimized for Q&A\n",
    "- **SimCSE**: Focuses on sentence similarity\n",
    "- **Instructor**: Task-specific embeddings\n",
    "\n",
    "**Key Point**: Embeddings trained for different tasks aren't interchangeable.\n",
    "\n",
    "**Reference**: Reimers & Gurevych (2019). \"Sentence-BERT\" (ACL)\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Dimensionality Matters\n",
    "\n",
    "#### Common Sizes\n",
    "- Word2Vec: 300 dimensions\n",
    "- BERT-base: 768 dimensions\n",
    "- BERT-large: 1024 dimensions\n",
    "- OpenAI ada-002: 1536 dimensions\n",
    "\n",
    "#### Storage Impact\n",
    "```\n",
    "384 dims: 1M docs Ã— 384 Ã— 4 bytes = ~1.5 GB\n",
    "1536 dims: 1M docs Ã— 1536 Ã— 4 bytes = ~6 GB\n",
    "```\n",
    "\n",
    "#### Dimensionality Reduction\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=256)\n",
    "reduced_embeddings = pca.fit_transform(original_embeddings)\n",
    "# Lose 5-15% accuracy but gain 3x speed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Context Length Limits\n",
    "\n",
    "#### Common Limits\n",
    "- BERT: 512 tokens (~400 words)\n",
    "- Longformer: 4096 tokens (~3000 words)\n",
    "- GPT-3 embeddings: 8191 tokens\n",
    "- Latest models: up to 32k+ tokens\n",
    "\n",
    "#### Handling Long Documents\n",
    "\n",
    "**Option 1: Truncation**\n",
    "```python\n",
    "embedding = model.encode(doc[:512])  # Loses information\n",
    "```\n",
    "\n",
    "**Option 2: Chunking + Averaging**\n",
    "```python\n",
    "chunks = [doc[i:i+512] for i in range(0, len(doc), 512)]\n",
    "chunk_embeddings = [model.encode(chunk) for chunk in chunks]\n",
    "doc_embedding = np.mean(chunk_embeddings, axis=0)\n",
    "```\n",
    "\n",
    "**Option 3: Max Pooling**\n",
    "```python\n",
    "doc_embedding = np.max(chunk_embeddings, axis=0)\n",
    "```\n",
    "\n",
    "**Option 4: Hierarchical Embedding**\n",
    "```python\n",
    "para_embeddings = [model.encode(para) for para in paragraphs]\n",
    "doc_embedding = meta_model.encode(para_embeddings)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Normalization is Critical\n",
    "\n",
    "Most similarity metrics assume normalized vectors.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "# Correct approach\n",
    "embedding = normalize(model.encode(doc).reshape(1, -1))[0]\n",
    "\n",
    "# With normalized vectors: cosine similarity = dot product\n",
    "similarity = np.dot(emb1, emb2)  # 10x faster\n",
    "```\n",
    "\n",
    "**Note**: Most modern models output normalized vectors by default, but always verify.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Domain Adaptation & Fine-tuning\n",
    "\n",
    "Pre-trained models may underperform on domain-specific text.\n",
    "\n",
    "#### When to Fine-tune\n",
    "- Domain-specific jargon (legal, medical, technical)\n",
    "- Unique document types (resumes, recipes, code)\n",
    "- Non-English languages\n",
    "- Performance isn't good enough\n",
    "\n",
    "#### Fine-tuning Example\n",
    "```python\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "train_examples = [\n",
    "    InputExample(texts=['MI', 'myocardial infarction'], label=1.0),\n",
    "    InputExample(texts=['MI', 'machine intelligence'], label=0.0),\n",
    "]\n",
    "\n",
    "model.fit(train_objectives=[(dataloader, loss)], epochs=1)\n",
    "```\n",
    "\n",
    "**Reference**: Guu et al. (2020). \"REALM: Retrieval-Augmented Language Model Pre-Training\" (ICML)\n",
    "\n",
    "---\n",
    "\n",
    "### 6. Multilingual Considerations\n",
    "\n",
    "Not all models handle multiple languages well.\n",
    "\n",
    "```python\n",
    "multi_model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')\n",
    "\n",
    "en_emb = multi_model.encode(\"Machine learning is powerful\")\n",
    "es_emb = multi_model.encode(\"El aprendizaje automÃ¡tico es poderoso\")\n",
    "\n",
    "similarity = cosine_similarity([en_emb], [es_emb])  # ~0.85\n",
    "```\n",
    "\n",
    "**Cross-lingual search**: Query in English, find documents in any language.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. Evaluation Metrics\n",
    "\n",
    "#### Intrinsic Evaluation (Embedding Quality)\n",
    "\n",
    "**Semantic Textual Similarity (STS)**\n",
    "```python\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# Compare model predictions with human judgments\n",
    "correlation, p_value = spearmanr(predicted_sims, human_scores)\n",
    "# Good models: 0.75-0.85\n",
    "```\n",
    "\n",
    "#### Extrinsic Evaluation (Task Performance)\n",
    "\n",
    "**Retrieval Metrics**\n",
    "```python\n",
    "def recall_at_k(query_emb, doc_embeddings, relevant_docs, k=10):\n",
    "    similarities = cosine_similarity([query_emb], doc_embeddings)[0]\n",
    "    top_k = np.argsort(similarities)[-k:]\n",
    "    retrieved = set(top_k) & set(relevant_docs)\n",
    "    return len(retrieved) / len(relevant_docs)\n",
    "```\n",
    "\n",
    "Common metrics: Recall@K, MRR (Mean Reciprocal Rank), NDCG\n",
    "\n",
    "**Reference**: Muennighoff et al. (2022). \"MTEB: Massive Text Embedding Benchmark\"\n",
    "\n",
    "---\n",
    "\n",
    "### 8. Cold Start & Embedding Drift\n",
    "\n",
    "#### Cold Start Problem\n",
    "New documents have no similar neighbors initially.\n",
    "\n",
    "**Solution**: Metadata filtering + hybrid search\n",
    "```python\n",
    "results = collection.query(\n",
    "    query_embeddings=[new_doc_emb],\n",
    "    where={\"category\": \"tech\", \"date\": {\"$gte\": \"2024-01-01\"}}\n",
    ")\n",
    "```\n",
    "\n",
    "#### Embedding Drift\n",
    "Embeddings become outdated as language/domain evolves.\n",
    "\n",
    "**Monitoring**\n",
    "```python\n",
    "def check_drift(old_embeddings, new_embeddings, threshold=0.1):\n",
    "    avg_old = np.mean(old_embeddings, axis=0)\n",
    "    avg_new = np.mean(new_embeddings, axis=0)\n",
    "    drift = 1 - cosine_similarity([avg_old], [avg_new])[0][0]\n",
    "    \n",
    "    if drift > threshold:\n",
    "        print(\"Significant drift - consider retraining\")\n",
    "    return drift\n",
    "```\n",
    "\n",
    "**Solution**: Periodic re-embedding or continuous fine-tuning.\n",
    "\n",
    "---\n",
    "\n",
    "### 9. Computational Costs\n",
    "\n",
    "Embedding isn't free - plan accordingly.\n",
    "\n",
    "#### Optimization Strategies\n",
    "\n",
    "**Batch Processing**\n",
    "```python\n",
    "# Good: batches (10x faster)\n",
    "embeddings = model.encode(docs, batch_size=32)\n",
    "\n",
    "# Bad: one at a time\n",
    "for doc in docs:\n",
    "    emb = model.encode(doc)\n",
    "```\n",
    "\n",
    "**GPU Acceleration**\n",
    "```python\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda')\n",
    "# 50-100x faster than CPU\n",
    "```\n",
    "\n",
    "**Model Distillation**\n",
    "- Teacher: BERT-large (1024 dims)\n",
    "- Student: MiniLM (384 dims)\n",
    "- Result: 3x faster, 90% accuracy\n",
    "\n",
    "**Caching**\n",
    "```python\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def get_embedding_cached(text):\n",
    "    return model.encode(text)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### 10. Security & Privacy\n",
    "\n",
    "Embeddings can leak information.\n",
    "\n",
    "#### Risk Example\n",
    "```python\n",
    "doc = \"Patient John Smith has HIV\"\n",
    "embedding = model.encode(doc)\n",
    "# Attacker with model can approximately reconstruct sensitive info\n",
    "```\n",
    "\n",
    "#### Mitigations\n",
    "- Differential privacy during training\n",
    "- Encrypted embeddings (homomorphic encryption)\n",
    "- Access control on vector databases\n",
    "- Anonymize before embedding\n",
    "\n",
    "**Reference**: Song & Shmatikov (2019). \"Auditing Data Provenance in Text-Generation Models\" (KDD)\n",
    "\n",
    "---\n",
    "\n",
    "### 11. When NOT to Use Embeddings\n",
    "\n",
    "Embeddings aren't always the answer.\n",
    "\n",
    "**Don't use when**:\n",
    "- Exact keyword matching needed (legal/compliance)\n",
    "- Highly structured data (use SQL)\n",
    "- Tiny datasets (<1000 docs) - simpler methods work\n",
    "- Real-time streaming (latency too high)\n",
    "- Explainability is critical (embeddings are black boxes)\n",
    "\n",
    "---\n",
    "\n",
    "## Critical Pre-Deployment Checklist\n",
    "\n",
    "âœ… **Model selection**: Right model for your domain/language?  \n",
    "âœ… **Dimensionality**: Balanced performance vs. storage?  \n",
    "âœ… **Normalization**: Vectors normalized for fast similarity?  \n",
    "âœ… **Context length**: Documents fit in model limits?  \n",
    "âœ… **Evaluation**: Measured on YOUR task, not generic benchmarks?  \n",
    "âœ… **Scalability**: Can handle your data growth?  \n",
    "âœ… **Costs**: Compute/storage budgeted?  \n",
    "âœ… **Monitoring**: Drift detection in place?  \n",
    "âœ… **Privacy**: Sensitive data protected?  \n",
    "\n",
    "---\n",
    "\n",
    "## Key Takeaways\n",
    "\n",
    "1. **Document embeddings convert text to numerical vectors** that capture semantic meaning\n",
    "2. **Pre-trained models work on raw text** - no prior processing needed\n",
    "3. **Applications go far beyond search** - classification, clustering, anomaly detection, recommendations\n",
    "4. **Vector databases enable scalable similarity search** at production scale\n",
    "5. **Choose the right method** - consider domain, language, task, and constraints\n",
    "6. **Dimensionality matters** - balance accuracy, speed, and storage\n",
    "7. **Handle long documents carefully** - chunking strategies matter\n",
    "8. **Fine-tune for domain-specific tasks** when pre-trained models underperform\n",
    "9. **Monitor and maintain** - embeddings can drift over time\n",
    "10. **Consider costs and privacy** - both computational and security implications\n",
    "\n",
    "---\n",
    "\n",
    "## Further Reading\n",
    "\n",
    "- Reimers & Gurevych (2019). \"Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks\" (ACL)\n",
    "- Guu et al. (2020). \"REALM: Retrieval-Augmented Language Model Pre-Training\" (ICML)\n",
    "- Muennighoff et al. (2022). \"MTEB: Massive Text Embedding Benchmark\" (arXiv)\n",
    "- Malkov & Yashunin (2018). \"Efficient and robust approximate nearest neighbor search using HNSW\"\n",
    "- Johnson et al. (2019). \"Billion-scale similarity search with GPUs\" - FAISS\n",
    "\n",
    "---\n",
    "\n",
    "*Document created: February 2026*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6fed104",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
